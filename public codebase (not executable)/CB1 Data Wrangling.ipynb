{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebase1: Data Wrangling\n",
    "\n",
    "The structured data is sourced from Sharadar's paid subscription and consists of (i) market data, (ii) financial data and (iii) metadata. The market data is used to calcuate the maximum 20 day rolling drawdown in the 1 year period following the filing of the annual report. The binary target data is defined as experiencing a positive event when this dardwown is greater than 80% and negative otherwise. \n",
    "\n",
    "The codebase is structured in the following sections:\n",
    "\n",
    "1. Data retrieval and early calculations\n",
    "2. Preprocessing\n",
    "3. Merging datasets into target-features data frame \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Data retrieval and early calculations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The market price database was too big to be loaded by API call and was instead bulk downloaded as a CSV file from the Quandl site.\n",
    "\n",
    "The below code uses the closing price of each equity to return the rolling 20 day max drawdowns on a daily basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-08-25 11:23:12.440870\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Converts daily equity prices from Sharadar database to rolling 20 day max\n",
    "drawdowns in dataframe format with columns as ticker and dates as index\n",
    "'''\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "#specify inputs\n",
    "window_dd = 20\n",
    "\n",
    "input_file_1 = 'daily_equity_prices.csv'\n",
    "output_file = 'monthly_rolling_20d_dd_whole_db.pickle'\n",
    "\n",
    "#read csv file of stock price data\n",
    "df_stocks = pd.read_csv(input_file_1, parse_dates=['date'])\n",
    "\n",
    "#pivot table and select closing prices only\n",
    "df_prices = df_stocks.pivot(index='date', columns='ticker', values='close')\n",
    "\n",
    "df_prices = df_prices.sort_index()\n",
    "#calculate max rolling 20 day drawdowns on rolling daily basis\n",
    "    #compute rolling dd\n",
    "df_dd = df_prices / df_prices.rolling(window_dd).max() -1\n",
    "df_dd = df_dd.applymap(lambda x: min(x,0))\n",
    "\n",
    "df_dd = df_dd.dropna(how='all', axis=1)\n",
    "\n",
    "#save dict to pickle\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(df_dd, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2 - t1)\n",
    "\n",
    "#runtime 3min30sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharadar also provides metadata for each equity ticker and this is downloaded via the Quandle API in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "output_file = 'meta_df_whole_db.pickle' \n",
    "\n",
    "#API Key\n",
    "quandl.ApiConfig.api_key = \"key\"\n",
    "\n",
    "#Pull data from quandl in df format\n",
    "df_meta = quandl.get_table('SHARADAR/TICKERS', paginate = True)     #all tickers and metadata\n",
    "\n",
    "\n",
    "#Wrangle Meta Table                                                    \n",
    "df_meta = df_meta[df_meta.table.eq('SF1')]                                              #filter by table 'SF1' \n",
    "df_meta.set_index('ticker', inplace=True)                                               #set ticker as index\n",
    "df_meta['CIK'] = df_meta['secfilings'].apply(lambda x: x[x.find('CIK=')+4:].strip())    #form new column for CIK refernece number (number as text)                                   \n",
    "df_meta.fillna(np.NaN, inplace=True)                                                    #fill None with NaN\n",
    "df_meta = df_meta.transpose()                                                           \n",
    "\n",
    "#save dataframe to file\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(df_meta, handle, protocol=pickle.HIGHEST_PROTOCOL)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to download the 10Ks from the SEC website. Given the highly imbalanced dataset, we use the rolling drawdown dataframe to find those tickers with maximum drawdowns over 80% and make sure these 10Ks are downloaded first. While company tickers can change for various reasons, the CIK number is unique and this links the drawdown and SEC 10K data through the metadata. Once the target companies have been specified and the CIK numbers retrieved, we ise the existing SEC downloader library to retrieve these annual statements to the local drive.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Takes in CIK number from metadata and drawdown dataframe to chose tickers for \n",
    "download from SEC website. Downloads to local drive and saves custom log.\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "from sec_edgar_downloader import Downloader\n",
    "from datetime import datetime as dt\n",
    "\n",
    "#Specify inout and outout files\n",
    "\n",
    "input_file_meta = 'meta_df_whole_db.pickle' \n",
    "input_file_dd = 'monthly_rolling_20d_dd_whole_db.pickle'\n",
    "output_log_file = '10k_dowload_logs.pickle'\n",
    "\n",
    "local_drive_destination = 'XXX'\n",
    "\n",
    "with open(input_file_meta, 'rb') as f_meta:\n",
    "        df_meta = pickle.load(f_meta)\n",
    "\n",
    "with open(input_file_dd, 'rb') as f_dd:\n",
    "        df_dd = pickle.load(f_dd)\n",
    "\n",
    "#find tickers with max dd >= 80%\n",
    "s_dd = df_dd.min(axis=0)\n",
    "mask_dd = s_dd <= -0.8\n",
    "pos_tickers = s_dd[mask_dd].index.tolist()\n",
    "neg_tickers = s_dd[~mask_dd].index.tolist()         \n",
    "tickers = pos_tickers + neg_tickers             #ensure pos_tickers downloaded first\n",
    "\n",
    "t0 = dt.now()\n",
    "print(t0)\n",
    "\n",
    "df_ticker_cik = df_meta.loc['CIK']\n",
    "\n",
    "#Initialize a downloader instance with specified destination\n",
    "dl = Downloader(local_drive_destination)\n",
    "\n",
    "# Initialize lists for custom log\n",
    "descr_list = []\n",
    "error_list = []\n",
    "\n",
    "#download all 10Ks of ticker after January 1997\n",
    "for idx, ticker in enumerate(tickers):                      \n",
    "    cik = df_ticker_cik[ticker]\n",
    "    try:\n",
    "        t1 = dt.now()\n",
    "        dl.get(\"10-K\", cik, after_date=\"19970101\")     \n",
    "        t2 = dt.now()\n",
    "        delta = t2-t1\n",
    "        descr = str(idx) + ' : ' + ticker + ' : ' + str(delta.seconds) + 'sec'\n",
    "        descr_list.append(descr)\n",
    "        print(descr)\n",
    "    except:\n",
    "        error_list.append(ticker)\n",
    "        descr = str(idx) + ' : ' + ticker + ' : ' + 'Error'\n",
    "        print(descr)\n",
    "        continue\n",
    "\n",
    "d_log = {'log': descr_list, 'error_codes': error_list}\n",
    "\n",
    "#save custom log to file\n",
    "with open(output_log_file, 'wb') as handle:\n",
    "    pickle.dump(d_log, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "t3 =dt.now()\n",
    "\n",
    "print(t3-t0)  \n",
    "\n",
    "#runtime overnight- stopped in morning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Preprocessing\n",
    "\n",
    "The 10Ks are pulled over a 20+ year period and are inconsistent in format (text, html, xbrl). Resultantly, the more general regex method is preferred for preprocessing. This is programmed as a function below. Stemming and lemmatization are intentionally excluded in order to leave the corpus as nuanced as possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags_char(text):\n",
    "    '''Takes in string and removes defined special characters  '''\n",
    "    \n",
    "    #Define special Chars\n",
    "    clean1 = re.compile('\\n')               \n",
    "    clean2 = re.compile('\\r')               \n",
    "    clean3 = re.compile('&nbsp;')           \n",
    "    clean4 = re.compile('&#160;')\n",
    "    clean5 = re.compile('  ')\n",
    "    #Define html tags\n",
    "    clean6 = re.compile('<.*?>')\n",
    "    #remove special characters and html tags\n",
    "    text = re.sub(clean1,' ', text)\n",
    "    text = re.sub(clean2,' ',text)  \n",
    "    text = re.sub(clean3,' ',text) \n",
    "    text = re.sub(clean4,' ',text) \n",
    "    text = re.sub(clean5,' ',text) \n",
    "    text = re.sub(clean6,' ',text) \n",
    "    # check spacing\n",
    "    final_text = ' '.join(text.split())  \n",
    "    \n",
    "    return final_text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to cleaning the 10Ks of special characters, the below code also pulls out document metadata from the text and creates a custom log to track failed documents. The most important metadata is the filing date which will be used to join the unstructured data with the target data. The program saves the output as a dictionary with each document specified by a concatenation of the ticker name and year as the primary key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Program processes downloaded 10ks with the following steps:\n",
    "    (i) maps SEC CIK number to stock exchange tickers needed for later comparison to financial data\n",
    "    (ii) Finds CIKs with two tickers to ensure 10k data stored for both\n",
    "    (iii) Walks through directory of downloaded CIKs converting CIK to ticker label\n",
    "    (iv) finds metadata section and extracts metadata for each 10k\n",
    "    (v) Find main 10k body and uses regex to remove html and other tags \n",
    "         before storing document as single string (10ks from 1997 have different\n",
    "         and inconsistent formats so regex preferred to html parser)\n",
    "    (vi) Ticker metadata added eg: sector, industry\n",
    "    (vii) user defined log and error list per ticker created and stored\n",
    "    (viii) Final output is dictionary with keys for log, errors and data.\n",
    "           Data is a nested dictionary with keys equal to ticker_name concateded with\n",
    "           year in label of 10k document, values are another dictionary including\n",
    "           document metadata, ticker metadata, and processed 10k text as string \n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from capstone_10k_functions import remove_html_tags_char\n",
    "\n",
    "t0 = dt.now()\n",
    "\n",
    "rootdir = 'XXX'  #for looping through raw 10ks \n",
    "input_file_1 = 'meta_df_whole_db.pickle'      #metadata  \n",
    "output_file = '10k_clean_dict.pickle'\n",
    "\n",
    "\n",
    "with open(input_file_1, 'rb') as f1:\n",
    "        v = pickle.load(f1)\n",
    "        \n",
    "\n",
    "#create cik to ticker df\n",
    "df_cik2tic = pd.DataFrame(v.loc['CIK',:])\n",
    "df_cik2tic = df_cik2tic.reset_index()\n",
    "\n",
    "\n",
    "#find duplicate tickers for single CIK\n",
    "bool_series = df_cik2tic['CIK'].duplicated(keep=False)\n",
    "df_dup_cik = df_cik2tic[bool_series].sort_values(by='CIK')\n",
    "df_dup_cik['CIK'] = df_dup_cik['CIK'].apply(lambda x: x.lstrip('0'))\n",
    "dup_n = len(df_dup_cik)\n",
    "\n",
    "if dup_n % 2 != 0:\n",
    "    print('Error: duplicate CIKs not an even number')\n",
    "else:\n",
    "    pass\n",
    "    \n",
    "index_list = [2*number for number in range(dup_n//2)]\n",
    "dict_dupes_cik2tic = {df_dup_cik.CIK.iloc[j]: (df_dup_cik.ticker.iloc[j], \n",
    "                                                df_dup_cik.ticker.iloc[j+1]) for j in index_list}\n",
    "\n",
    "#Remove duplicates from primary cik to ticker df \n",
    "df_cik2tic = df_cik2tic[~bool_series]\n",
    "df_cik2tic['CIK'] = df_cik2tic['CIK'].apply(lambda x: x.lstrip('0'))\n",
    "df_cik2tic = df_cik2tic.set_index('CIK')\n",
    "\n",
    "\n",
    "#\n",
    "n = 0\n",
    "\n",
    "d_all = {}          #final outout dict\n",
    "descr_list = []     #description list for live debugging\n",
    "error_list = []     #list for error logging\n",
    "\n",
    "#Walk thorugh 10k downloads for primary non-duplicated ciks\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        \n",
    "        t1 = dt.now()       #start clock for each document \n",
    "        \n",
    "        #find cik number from filename\n",
    "        subdir_str = str(subdir)\n",
    "        start_sub= subdir_str.find('filings') + 8\n",
    "        end_sub = subdir_str.find('10-K') - 1\n",
    "        cik = subdir_str[start_sub:end_sub]\n",
    "        \n",
    "        #find year in name of document (label in file, may not reflect report yr)\n",
    "        old_fname_str = str(file)\n",
    "        start_fn = old_fname_str.find('-') +1\n",
    "        end_fn = start_fn + 2\n",
    "        year_fn = old_fname_str[start_fn:end_fn]            \n",
    "     \n",
    "        #map cik to ticker for renaming & check if cik map unique\n",
    "        try:\n",
    "            ticker = df_cik2tic.loc[cik, 'ticker']\n",
    "            key_all = ticker + '_' + year_fn\n",
    "            dupe_flag = False\n",
    "        except:\n",
    "            list_ticker = dict_dupes_cik2tic[cik]\n",
    "            key_all_0 = list_ticker[0] + '_' + year_fn\n",
    "            key_all_1 = list_ticker[1] + '_' + year_fn\n",
    "            dupe_flag = True        \n",
    "        \n",
    "        #finally ready to open and work with document\n",
    "        filename = os.path.join(subdir, file)\n",
    "        \n",
    "        n += 1      #counter for live print debugging\n",
    "       \n",
    "        try:\n",
    "            with open (filename, 'r') as file:    \n",
    "    \n",
    "                file_str = file.read()                      #read file into memory\n",
    "    \n",
    "                end_1 = file_str.find('<SEQUENCE>2')        #start section follow main 10k\n",
    "                start = file_str.find('<SEQUENCE>1')        #start of 10k / end of metadata\n",
    "                end = file_str.find('</DOCUMENT>')          #end of 10k if before end_1\n",
    "                \n",
    "                #Extract metadata from document\n",
    "                meta_text = file_str[:start]                #meta data section\n",
    "    \n",
    "                doc_metadata = ['ACCESSION NUMBER:', 'CONFORMED SUBMISSION TYPE:',                #metadata labels in document (order important)\n",
    "                                'PUBLIC DOCUMENT COUNT:', 'CONFORMED PERIOD OF REPORT:', \n",
    "                                'FILED AS OF DATE:', 'DATE AS OF CHANGE:']\n",
    "    \n",
    "                doc_key_names = ['Accession_#', 'Type', 'Doc_Count','Period',       #key names for metadata\n",
    "                                 'Filed_Date', 'Change_Date']\n",
    "                \n",
    "                pos_start = [meta_text.find(label) for label in doc_metadata]               #start pos meta data label\n",
    "                pos_end = [meta_text.find(label) + len(label) for label in doc_metadata]    #end pos meta data label\n",
    "                doc_meta_values = [meta_text[pos_end[j]:pos_start[j+1]].strip()             #metadata value between end label and beg next label \n",
    "                                       for j in range(len(doc_metadata)-1)]  \n",
    "                doc_meta_values[-1] = doc_meta_values[-1][:8]                               #last label manual as no next label\n",
    "                \n",
    "                #define 10k body and clean of html / text / xbrl etc.\n",
    "                text = file_str[start:end_1]            #define sequence 1\n",
    "                text = text[:end]                       #end sequence 1 doc\n",
    "                \n",
    "                #remove html tags and special chars\n",
    "                text = remove_html_tags_char(text)\n",
    "                \n",
    "                #create main dict with 10k metadata and 10k text as string\n",
    "                d = dict(zip(doc_key_names, doc_meta_values))\n",
    "                d.update({'Text': text})\n",
    "                \n",
    "                #add ticker sector / industry metadata to main dict\n",
    "                ticker_meta_short = ['name', 'sicsector', 'sicindustry', 'famasector',  #define metadata of interest\n",
    "                                     'famaindustry', 'sector', 'industry']      #for later categorical analysis\n",
    "                df_meta_short = v[ticker][ticker_meta_short]           #extract metadata \n",
    "                d.update(df_meta_short.to_dict())                           #add to main dict\n",
    "\n",
    "                \n",
    "                #treat for cik duplicate or not to populate final dict with\n",
    "                #logs and errors\n",
    "                if dupe_flag == False:                          #no dupe, write 10k to unique ticker\n",
    "                    d_all.update({key_all: d})\n",
    "                    \n",
    "                    t2 = dt.now()\n",
    "                    delta = t2 - t1\n",
    "                    descr = str(n) + ' : ' + key_all + ' : ' + str(delta.microseconds/1000000)  #description for log\n",
    "                    descr_list.append(descr)                                                    #append to log\n",
    "                    print(descr)                                                                #print to screen for live record\n",
    "                else:\n",
    "                    d_all.update({key_all_0: d, key_all_1: d})  #if dupe, write 10k to both tickers\n",
    "                \n",
    "                    t2 = dt.now()\n",
    "                    delta = t2 - t1\n",
    "                    descr = str(n) + ' : ' + key_all_0 + ' | ' + key_all_1 + ' : ' + str(delta.microseconds/1000000)    #description for log\n",
    "                    descr_list.append(descr)                #append to log\n",
    "                    print(descr)                            #print to screen for live record\n",
    "                    \n",
    "        #if metadata and 10k wrangle fails, record error            \n",
    "        except:  \n",
    "                 \n",
    "            try:\n",
    "                if dupe_flag == False:\n",
    "                    error_list.append(key_all)                           #write error to list\n",
    "                else:                                                       \n",
    "                    error_list.append(key_all_0 + ' | ' + key_all_1)    #if fail on duplicate, make sure to record both tickers\n",
    "                        \n",
    "                t2 = dt.now()            \n",
    "                delta = t2 - t1\n",
    "                descr = str(n) + ' : ' + key_all + ' : ' + 'Error'      #print to screen for live record of error\n",
    "                descr_list.append(descr)                                #append to log\n",
    "                print(descr)\n",
    "                continue\n",
    "        \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "d_all.update({'log' : descr_list, 'error_codes': error_list})       #write log and errors to final dict\n",
    "\n",
    "with open(output_file, 'wb') as handle:                             #save final dict as pickle\n",
    "    pickle.dump(d_all, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "t3 = dt.now()   \n",
    "print(t3-t0)    \n",
    "\n",
    "#runtime 30mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary is converted to a data frame format where the concatenated reference is dropped and the \"tickers\" and \"Filed_Date\" columns become the unique identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Convert dictionary of processed 10k statements dictionary with ticker as keys and value as a dataframe\n",
    "\"\"\"\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "input_file = '10k_clean_dict.pickle'\n",
    "output_file = '10k_clean_df.pickle'\n",
    "\n",
    "#Load clean dictionary \n",
    "with open(input_file, 'rb') as f:\n",
    "        z = pickle.load(f).copy()\n",
    "        \n",
    "#delete keys that are not related to k10 data        \n",
    "del z['log']                    \n",
    "del z['error_codes']\n",
    "\n",
    "#create dictionary with ticker as key and a list of all annual dictionaries as values\n",
    "d_temp = {}\n",
    "for k, v in z.items():\n",
    "    end = k.find('_')                               #find ticker (eg: AAPL) from long key name (eg: AAPL_18)\n",
    "    ticker = k[:end]\n",
    "    \n",
    "    if ticker not in list(d_temp.keys()):           \n",
    "        d_temp.update({ticker: [v]})            #if key hasn't appeared yet, initialise with list for value\n",
    "    else:\n",
    "        d_temp[ticker].append(v)                #if key has appeared, append value to list\n",
    "        \n",
    "\n",
    "#convert ticker dictionaries to dataframes \n",
    "df_final = pd.DataFrame()\n",
    "for k, v in d_temp.items():    \n",
    "    df = pd.DataFrame.from_dict(v, orient='columns')    \n",
    "    df['ticker']= k                                                                    #add ticker column\n",
    "    df['file_month_date'] = pd.to_datetime(df['Filed_Date'], errors = 'coerce')\n",
    "    df['file_month_date'] = df['file_month_date'] + pd.offsets.MonthEnd(0)\n",
    "    #add column year of of statement plus 1\n",
    "    df = df.sort_values(['ticker', 'file_month_date'])    \n",
    "    \n",
    "    df_final = df_final.append(df)                                                        #set key (ticker) and value (df) for final dictionary\n",
    "\n",
    "\n",
    "with open(output_file, 'wb') as handle:                                     #save final dictionary as pickle file\n",
    "    pickle.dump(df_final, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#runtime 10min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Merging the Datasets\n",
    "\n",
    "Merging the 10K and drawdown dataframes will result in a loss of information. for example, there will be some price tickers with price history but no recorded filings or with incomplete filings. The below code inner joins the dataframes on the ticker and Filed_Date columns and calaculates the maximum of the drawdowns in the year following the Filing as the target variable.\n",
    "\n",
    "The function for computing this drawdown is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_dd_period(s, date1, date2, window=20):\n",
    "    \"\"\"finds the max drawdown of the 20 day rolling dd series between the dates\"\"\"\n",
    "    s_dd = pd.Series(s[window-1:].values, index=s.index[:-(window-1)], name=s.name)\n",
    "\n",
    "    mask = (s_dd.index > date1) & (s_dd.index <= date2)\n",
    "    \n",
    "    max_dd = s_dd[mask].min()\n",
    "    \n",
    "    return max_dd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the merge is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from capstone_10k_functions import find_max_dd_period\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "\n",
    "\n",
    "input_text = '10k_clean_df.pickle'\n",
    "input_dd = 'monthly_rolling_20d_dd_whole_db.pickle'\n",
    "\n",
    "output_file = 'dict_10k_matched_dd.pickle'\n",
    "\n",
    "\n",
    "\n",
    "with open(input_text, 'rb') as f_text:\n",
    "        df_text = pickle.load(f_text)\n",
    "\n",
    "#set to datetime format\n",
    "df_text['Filed_Date'] = pd.to_datetime(df_text['Filed_Date'], errors = 'coerce')\n",
    "#define 10k text df for later merging\n",
    "df_text_actual = df_text[['ticker', 'Filed_Date', 'Text']]\n",
    "df_text_actual.columns = ['ticker_', 'Filed_Date', 'Text']\n",
    "#non text data to carry through calcs before merge\n",
    "df_text = df_text[['ticker', 'Filed_Date', 'sector', 'sicsector']]\n",
    "\n",
    "#10K tickers to list\n",
    "tickers_text = set(df_text['ticker'].tolist())   #len = 4,482\n",
    "\n",
    "\n",
    "with open(input_dd, 'rb') as f_dd:\n",
    "        df_dd = pickle.load(f_dd)\n",
    "        \n",
    "#drawdown tickers to list\n",
    "tickers_dd = set(df_dd.columns.tolist())        #len = 16,973\n",
    "\n",
    "#find intersection of tickers across the dataframes\n",
    "tickers = tickers_text.intersection(tickers_dd)   #len = 4,456\n",
    "tickers = list(tickers)\n",
    "\n",
    "\n",
    "#match 10k file date with max 20d dd over next 12 months\n",
    "counter=0\n",
    "#loop through tickers\n",
    "for code in tickers:\n",
    "    s_dd = df_dd[code]\n",
    "    \n",
    "    #event flag column\n",
    "    ticker_dd_flag = (s_dd.min() <= -0.8)*1\n",
    "    \n",
    "    df_10k = df_text[df_text.ticker == code].reset_index(drop=True)\n",
    "    df_10k.columns = ['ticker_', 'Filed_Date', 'sector', 'sicsector']\n",
    "    \n",
    "    #info for meta df\n",
    "    sector =df_10k['sector'][0]\n",
    "    sic_sector =df_10k['sicsector'][0]\n",
    "    #custm sector category\n",
    "    custom_sector = str(sector) + ' : ' + str(sic_sector)\n",
    "    \n",
    "    meta_dict = {'sector': sector, 'sic_sector': sic_sector, \n",
    "                       'custom_sector': custom_sector, 'ticker_dd_flag': \n",
    "                           ticker_dd_flag }\n",
    "    df_meta = pd.DataFrame(meta_dict, index = [code])\n",
    "    \n",
    "    #loop through years\n",
    "    for row in range(len(df_10k)):\n",
    "                    \n",
    "                    #find max dd over next 1 and 2 years\n",
    "                     start = df_10k.loc[row, 'Filed_Date']\n",
    "                     end_1yr = start + DateOffset(months=12)\n",
    "                     end_2yr = start + DateOffset(months=24)\n",
    "                     max_dd_1yr = find_max_dd_period(s_dd, start, end_1yr, window=20)\n",
    "                     max_dd_2yr = find_max_dd_period(s_dd, start, end_2yr, window=20)\n",
    "                     df_10k.loc[row, 'max_dd_1yr'] = max_dd_1yr\n",
    "                     df_10k.loc[row, 'max_dd_2yr'] = max_dd_2yr\n",
    "                     df_10k.loc[row, 'year_dd_flag'] = (max_dd_1yr <= -0.8)*1\n",
    "                     \n",
    "    #add_cumulative_year_dd_flag (incl)\n",
    "    df_10k['cum_year_dd_flag'] = df_10k['year_dd_flag'].expanding().max()\n",
    "    \n",
    "    df_10k = df_10k.dropna()\n",
    "    \n",
    "    #if emoty then skip ticker\n",
    "    if df_10k.empty:\n",
    "        counter +=1\n",
    "        continue\n",
    "    else:\n",
    "        pass\n",
    "    #if first iteration, initialize df for concate over next loops\n",
    "    if counter == 0:\n",
    "        df_final = df_10k\n",
    "        df_meta_final = df_meta\n",
    "    else:\n",
    "        df_final = pd.concat([df_final, df_10k])\n",
    "        df_meta_final = pd.concat([df_meta_final, df_meta])\n",
    "        \n",
    "    counter += 1\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "#\n",
    "df_final = df_final.merge(df_text_actual, on=['ticker_','Filed_Date'], how='inner')\n",
    "\n",
    "dict_final = {'matched_df_10k_dd': df_final, 'matched_df_10k_dd_meta': df_meta_final}    #len = 4,365 tickers / 38,807 docs\n",
    "\n",
    "\n",
    "#save dict to pickle\n",
    "with open(output_file, 'wb') as handle:                                     \n",
    "    pickle.dump(dict_final, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to convert the 10Ks from a text string in a column of a dataframe to a features set in td_idf matrix form which makes use of the following function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_corpus(text_series, vectorizer_func, min_df, max_df, ngram_range):\n",
    "    '''vectorize corpus with specified vectorizer (tdidf or count) \n",
    "    and parameters'''\n",
    "    \n",
    "    vectorizer = vectorizer_func(min_df=min_df, max_df=max_df)\n",
    "    vectors = vectorizer.fit_transform(text_series)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    #wrap vectors in sparse dataframe and label\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(vectors, columns = feature_names)\n",
    "    \n",
    "    #drop null columns\n",
    "    df_test = df[:5]\n",
    "    null_columns = df_test.columns[df_test.isnull().any()]\n",
    "    df = df.drop(null_columns, axis=1)\n",
    "    \n",
    "    dict_answer ={'df_wv': df, 'vectorizer': vectorizer}\n",
    "    \n",
    "    return dict_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the vecorization and formation of the trainning and test sets across the expanding time-series cross validation folds is given by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "vectorize corpus and create target-features matrix across validation folds \n",
    "using expanding windows for time series data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from capstone_10k_functions import vectorize_corpus\n",
    "from datetime import datetime as dt\n",
    "\n",
    "t1 = dt.now()\n",
    "print(t1)\n",
    "\n",
    "\n",
    "input_file = 'dict_10k_matched_dd.pickle'\n",
    "\n",
    "\n",
    "vector_func = TfidfVectorizer    \n",
    "func_name = 'TfidfVectorizer'   #['TfidfVectorizer', 'CountVectorizer']\n",
    "\n",
    "hold_out_set_start = 2015\n",
    "\n",
    "k_ratio = 0.2\n",
    "min_df = 15\n",
    "min_df_grid = [min_df]\n",
    "\n",
    "max_df = 0.5\n",
    "ngram = (1,2)\n",
    "ngram_name = 'bigram'\n",
    "\n",
    "label_cv = ['cv1', 'cv2', 'cv3', 'cv4']\n",
    "\n",
    "\n",
    "\n",
    "with open(input_file , 'rb') as f:\n",
    "        d_data = pickle.load(f)\n",
    "df = d_data['matched_df_10k_dd']\n",
    "\n",
    "df = df.sort_values(\"Filed_Date\")\n",
    "\n",
    "\n",
    "##Define validation sets\n",
    "mask_hold_out = df['Filed_Date'].dt.year >= hold_out_set_start\n",
    "df_v = df[~mask_hold_out]\n",
    "size = df_v.shape[0]\n",
    "n = int(k_ratio*size)\n",
    "k_stops = [n, 2*n, 3*n, 4*n, size]\n",
    "\n",
    "\n",
    "    \n",
    "#Generate df master (word vector / vectorizer) sets for each cv fold\n",
    "\n",
    "\n",
    "for idx_cv, label in enumerate(label_cv):\n",
    "    \n",
    "    output_filename = label + '_' + func_name + '_' +'min_df_' + str(min_df) +'_' + ngram_name + '.pickle'\n",
    "    dict_cv = {}\n",
    "    \n",
    "    print(label)\n",
    "    \n",
    "    stop_train = k_stops[idx_cv]\n",
    "    stop_test = k_stops[idx_cv + 1]\n",
    "    df_test = df[stop_train: stop_test]\n",
    "    df_train = df[:stop_train ]\n",
    "    \n",
    "    #format training data\n",
    "    df_train_text = df_train[['ticker_','Filed_Date', 'Text']]\n",
    "    df_train_other = df_train.drop('Text', axis=1)\n",
    "    df_train_other.columns = ['ticker_', 'Filed_Date', 'sector_', 'sic_sector', \n",
    "                        'max_dd_1yr', 'max_dd_2yr', 'year_dd_flag', \n",
    "                        'cum_year_dd_flag']\n",
    "    df_train_other['custom_sector'] = str(df_train_other['sector_']) + ' : ' + str(df_train_other['sic_sector'])\n",
    "\n",
    "    #format testing data\n",
    "    df_test_text = df_test[['ticker_','Filed_Date', 'Text']]\n",
    "    df_test_other = df_test.drop('Text', axis=1)\n",
    "    df_test_other.columns = ['ticker_', 'Filed_Date', 'sector_', 'sic_sector', \n",
    "                        'max_dd_1yr', 'max_dd_2yr', 'year_dd_flag', \n",
    "                        'cum_year_dd_flag']\n",
    "    df_test_other['custom_sector'] = str(df_test_other['sector_']) + ' : ' + str(df_test_other['sic_sector'])\n",
    "        \n",
    "\n",
    "    for min_df in min_df_grid: \n",
    "        print(min_df)\n",
    "        \n",
    "        #name for cv dictionary specified by min_df value\n",
    "        key_name = 'min_df_' + str(min_df)\n",
    "        \n",
    "        #vectorize corpus and assign word vector and vectorizer\n",
    "        function = vectorize_corpus(df_train_text['Text'], vector_func, min_df, \n",
    "                                            max_df,ngram)\n",
    "        X = function['df_wv']\n",
    "        vectorizer = function['vectorizer']\n",
    "        \n",
    "        #Transform training data into df_master format\n",
    "        vocab = X.columns.tolist()\n",
    "        X['Filed_Date'] = df_train_text['Filed_Date'].values\n",
    "        X['ticker_'] = df_train_text['ticker_'].values\n",
    "                        \n",
    "        df_train_master = df_train_other.merge(X, on=['ticker_','Filed_Date'], how='inner')\n",
    "        \n",
    "        #Transform test data into df master format\n",
    "        arr_test_transform = vectorizer.transform(df_test_text['Text'])\n",
    "        df_test_transform = pd.DataFrame.sparse.from_spmatrix(arr_test_transform,\n",
    "                                                           columns = vocab)\n",
    "        df_test_transform['Filed_Date'] = df_test_text['Filed_Date'].values\n",
    "        df_test_transform['ticker_'] = df_test_text['ticker_'].values\n",
    "        \n",
    "        \n",
    "        df_test_master = df_test_other.merge(df_test_transform, \n",
    "                                             on=['ticker_','Filed_Date'], \n",
    "                                                                 how='inner')\n",
    "        \n",
    "            \n",
    "        dict_final = {'df_test_master': df_test_master, 'df_train_master': df_train_master}\n",
    "                \n",
    "        dict_cv[key_name] = dict_final\n",
    "           \n",
    "    with open(output_filename, 'wb') as handle:                                     \n",
    "        pickle.dump(dict_cv, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "t2 = dt.now()\n",
    "print(t2)\n",
    "print(t2-t1)\n",
    "              \n",
    "                \n",
    "    #runtime 2hrs30mins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
